{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9df6b71a-da03-4eff-a363-f25bc9c862ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import six\n",
    "import collections\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa8ab99-b3c3-449b-befa-93aca36f8784",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f069b85b-7455-4fca-9094-51ceaa38f44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import config\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a975ba3-b520-400d-89cc-7f7c542dc03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_sequence(fname):\n",
    "    usernum = 0\n",
    "    itemnum = 0\n",
    "    Item = defaultdict(list)\n",
    "    item_train = {}\n",
    "    item_valid = {}\n",
    "    item_test = {}\n",
    "    \n",
    "    user_num = []\n",
    "    item_num = []\n",
    "    \n",
    "    # assume user/item index starting from 1\n",
    "    f = open(fname, 'r')\n",
    "    for line in f:\n",
    "        i, u = line.rstrip().split(' ')\n",
    "        u = int(u)\n",
    "        i = int(i)\n",
    "        usernum = max(u, usernum)\n",
    "        itemnum = max(i, itemnum)\n",
    "        Item[i].append(u)\n",
    "    \n",
    "        user_num.append(u)\n",
    "        item_num.append(i)\n",
    "    \n",
    "    user_num = list(dict.fromkeys(user_num))\n",
    "    item_num = list(dict.fromkeys(item_num))\n",
    "    \n",
    "    for item in Item:\n",
    "        nfeedback = len(Item[item])\n",
    "        if nfeedback < 3:\n",
    "            item_train[item] = Item[item]\n",
    "            item_valid[item] = []\n",
    "            item_test[item] = []\n",
    "        else:\n",
    "            item_train[item] = Item[item][:-2]\n",
    "            item_valid[item] = []\n",
    "            item_valid[item].append(Item[item][-2])\n",
    "            item_test[item] = []\n",
    "            item_test[item].append(Item[item][-1])\n",
    "    return [item_train, item_valid, item_test, len(user_num), len(item_num)]\n",
    "\n",
    "def item_sequence(fname):\n",
    "    usernum = 0\n",
    "    itemnum = 0\n",
    "    User = defaultdict(list)\n",
    "    user_train = {}\n",
    "    user_valid = {}\n",
    "    user_test = {}\n",
    "    \n",
    "    user_num = []\n",
    "    item_num = []\n",
    "    \n",
    "    # assume user/item index starting from 1\n",
    "    f = open(fname, 'r')\n",
    "    for line in f:\n",
    "        u, i = line.rstrip().split(' ')\n",
    "        u = int(u)\n",
    "        i = int(i)\n",
    "        usernum = max(u, usernum)\n",
    "        itemnum = max(i, itemnum)\n",
    "        User[u].append(i)\n",
    "    \n",
    "        user_num.append(u)\n",
    "        item_num.append(i)\n",
    "    \n",
    "    user_num = list(dict.fromkeys(user_num))\n",
    "    item_num = list(dict.fromkeys(item_num))\n",
    "    \n",
    "    for user in User:\n",
    "        nfeedback = len(User[user])\n",
    "        if nfeedback < 3:\n",
    "            user_train[user] = User[user]\n",
    "            user_valid[user] = []\n",
    "            user_test[user] = []\n",
    "        else:\n",
    "            user_train[user] = User[user][:-2]\n",
    "            user_valid[user] = []\n",
    "            user_valid[user].append(User[user][-2])\n",
    "            user_test[user] = []\n",
    "            user_test[user].append(User[user][-1])\n",
    "    return [user_train, user_valid, user_test, len(user_num), len(item_num)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48d7387b-bc6b-45cd-954c-91fb296c78c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_user_seq = user_sequence('./HybridBERT4Rec/data/ml-1m-item.txt') \n",
    "dataset_item_seq = item_sequence('./HybridBERT4Rec/data/ml-1m-item.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d36b1f28-e67c-437b-81ad-469828229917",
   "metadata": {},
   "outputs": [],
   "source": [
    "[item_train, item_valid, item_test, usernum, itemnum] = dataset_user_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2040aef4-5213-4edd-8b02-f9782991c261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average sequence length: 268.00\n",
      "max:3426, min:1\n",
      "len_train:3706, len_valid:3706, len_test:3706, usernum:6040, itemnum:3706\n"
     ]
    }
   ],
   "source": [
    "cc = 0.0\n",
    "max_len = 0\n",
    "min_len = 100000\n",
    "\n",
    "\n",
    "for i in item_train:\n",
    "    cc += len(item_train[i])\n",
    "    max_len = max(len(item_train[i]), max_len)\n",
    "    min_len = min(len(item_train[i]), min_len)\n",
    "    # if(min_len==4):\n",
    "    #     print(u)\n",
    "    #     print(user_train[u])\n",
    "    #     break\n",
    "\n",
    "\n",
    "print('average sequence length: %.2f' % (cc / len(item_train)))\n",
    "print('max:{}, min:{}'.format(max_len, min_len))\n",
    "\n",
    "print('len_train:{}, len_valid:{}, len_test:{}, usernum:{}, itemnum:{}'.\n",
    "    format(\n",
    "    len(item_train),\n",
    "    len(item_valid), len(item_test), usernum, itemnum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e89d8b-49bb-4787-b14b-8a38418e8034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put validate into train\n",
    "for i in item_train:\n",
    "    if i in item_valid:\n",
    "        item_train[i].extend(item_valid[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73e4fc62-bc86-4b84-87c8-bf91d3628ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from HybridBERT4Rec.vocab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64d95451-38e6-46aa-bacd-11824bafba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = FreqVocab(item_train)  # ใช้ vocab แค่ item_test_data เพราะมันมีข้อมูลของทั้ง train และ test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b6f81a5-e4c8-4979-9085-ca2a352cf6be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6040"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.get_item_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ddad605-aac0-4b9c-b701-65eb20902730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cdba5a15-9484-4529-9ec8-3dc95186215d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get the max index of the data\n",
    "item_train_data = {\n",
    "    str(k): [str(user) for user in v]\n",
    "    for k, v in item_train.items() if len(v) > 0\n",
    "}\n",
    "item_test_data = {\n",
    "    str(i):\n",
    "        [str(user) for user in (item_train[i] + item_test[i])]\n",
    "    # for i in item_train if len(item_train[i]) > 0 and len(item_test[i]) > 0\n",
    "    for i in item_train if len(item_train[i]) > 0\n",
    "}\n",
    "random_seed = 12345\n",
    "rng = random.Random(random_seed)\n",
    "\n",
    "vocab = FreqVocab(item_test_data)  # ใช้ vocab แค่ item_test_data เพราะมันมีข้อมูลของทั้ง train และ test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fb7053-e9cf-4ff5-bacf-9079f45ab7b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ee6705b-bb1e-48b6-aec0-93812ff97562",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulory = vocab.get_items()\n",
    "random.shuffle(vocabulory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "db2aaf03-e7e5-4153-8da5-f3305f07933e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulory_lines = \"\\n\".join(vocabulory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5e849752-4959-4404-bf3b-bacf49a8041c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('HybridBERT4Rec/vocab.txt','w') as f:\n",
    "    f.write(vocabulory_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "29b95d2b-98d3-4ad5-b631-4926815f13f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForMaskedLM, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0a52d29f-b87a-43b3-9512-615e7894ef1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "de19dfe5-d9f5-4043-95fc-40b380900f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create and load your custom tokenizer with the 6040 vocabulary\n",
    "custom_vocab_path = \"HybridBERT4Rec/vocab.txt\"\n",
    "tokenizer = BertTokenizer.from_pretrained(custom_vocab_path)\n",
    "\n",
    "# # Step 2: Load the corresponding pre-trained model for the custom vocabulary\n",
    "# model = TFBertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "# # Now you can use the tokenizer and model as usual for training or fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f24bdc6b-5913-4f15-82b0-e799d07270dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load the pre-trained model and get its configuration\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = TFBertForMaskedLM.from_pretrained(model_name)\n",
    "config = model.config\n",
    "\n",
    "# Step 3: Update the model configuration with the custom vocabulary size\n",
    "config.vocab_size = len(tokenizer)\n",
    "\n",
    "# Step 4: Load the model again with the updated configuration\n",
    "model = TFBertForMaskedLM.from_config(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ec69fb30-eab1-4e09-a6a0-825924118bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vals = []\n",
    "for k, val in item_train_data.items():\n",
    "    all_vals.append(\" \".join(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "12c647ce-07ef-4908-bef3-3fc61fe28133",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 200\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_dataset = tokenizer(all_vals, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2380d1a2-30a5-4cd0-97e4-71083e3e2163",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_dataset['input_ids'].numpy()\n",
    "\n",
    "# Create labels by copying input_ids\n",
    "labels = tokenized_dataset[\"input_ids\"].numpy().copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "73c3336a-17c0-4241-a1c8-21d35f687e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_probability = 0.15\n",
    "tokenized_dataset_input_ids = tokenized_dataset[\"input_ids\"].numpy().copy()\n",
    "for i, row in enumerate(tokenized_dataset[\"attention_mask\"].numpy()):\n",
    "    # actual tokens\n",
    "    actual_tokens = tokenized_dataset_input_ids[i]\n",
    "    cls_token_pos = np.argwhere((actual_tokens == tokenizer.cls_token_id))\n",
    "    sep_token_pos = np.argwhere((actual_tokens == tokenizer.sep_token_id))\n",
    "    \n",
    "    # FINDING indices with non-padding tokens, CLS token and SEP token\n",
    "    non_pad_ids = np.argwhere(row == 1).flatten()     \n",
    "    non_pad_ids_list = non_pad_ids.tolist()\n",
    "    non_pad_ids_list.remove(cls_token_pos)\n",
    "    non_pad_ids_list.remove(sep_token_pos)\n",
    "    non_pad_ids = np.array(non_pad_ids_list)\n",
    "    random.shuffle(non_pad_ids)\n",
    "    mask_indices = non_pad_ids[:num_mask_tokens]\n",
    "\n",
    "    num_mask_tokens = int(len(non_pad_ids) * mask_probability)\n",
    "    \n",
    "    # Replace masked tokens with [MASK] token id\n",
    "    tokenized_dataset_input_ids[i][mask_indices] = tokenizer.mask_token_id\n",
    "\n",
    "tokenized_dataset_input_ids_tensors = tf.convert_to_tensor(tokenized_dataset_input_ids)\n",
    "tokenized_dataset['input_ids'] = tokenized_dataset_input_ids_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "3fe0f378-c16a-4074-b5b3-3979ac9c206a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = (tokenized_dataset['input_ids'], tokenized_dataset[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9e6299-af07-4051-beb3-454c04f44438",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fbeaa8-c971-4c1f-8007-63fb10c37c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Main function to train the model\n",
    "# def main():\n",
    "#     # Step 1: Load the tokenizer and model\n",
    "#     tokenizer, model = load_tokenizer_and_model()\n",
    "\n",
    "#     # Step 2: Prepare your custom dataset for masked language modeling\n",
    "#     dataset_path = 'path/to/your/custom_dataset.txt'\n",
    "#     inputs, labels = prepare_dataset(dataset_path, tokenizer)\n",
    "\n",
    "#     # Step 3: Define the loss function and compile the model\n",
    "#     model = compile_model(model)\n",
    "\n",
    "#     # Step 4: Fine-tune the model on your custom dataset\n",
    "#     batch_size = 16\n",
    "#     num_epochs = 3\n",
    "#     model = train_model(model, inputs, labels, batch_size, num_epochs)\n",
    "\n",
    "#     # Step 5: Save the trained model and tokenizer\n",
    "#     save_dir = 'path/to/save/model/'\n",
    "#     save_model_and_tokenizer(model, tokenizer, save_dir)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "8ea18440-a147-457a-8ca4-b010e13101de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_C = compile_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "8a58a33c-0882-4d05-94a0-d5273903a91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard('logdIR', histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86f63da-19bc-4ec8-a43f-888252d64f62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e35096e4-cc2d-4c3b-9591-0d6b03456847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 70/464 [===>..........................] - ETA: 1:04:35 - loss: 0.9722"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[163], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtensorboard_callback\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\DataScienceAnywhere\\Udemy\\bert_tutorial\\bert_venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mD:\\DataScienceAnywhere\\Udemy\\bert_tutorial\\bert_venv\\lib\\site-packages\\keras\\src\\engine\\training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1740\u001b[0m ):\n\u001b[0;32m   1741\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1742\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1744\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mD:\\DataScienceAnywhere\\Udemy\\bert_tutorial\\bert_venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mD:\\DataScienceAnywhere\\Udemy\\bert_tutorial\\bert_venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mD:\\DataScienceAnywhere\\Udemy\\bert_tutorial\\bert_venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mD:\\DataScienceAnywhere\\Udemy\\bert_tutorial\\bert_venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\DataScienceAnywhere\\Udemy\\bert_tutorial\\bert_venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mD:\\DataScienceAnywhere\\Udemy\\bert_tutorial\\bert_venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[1;32mD:\\DataScienceAnywhere\\Udemy\\bert_tutorial\\bert_venv\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
      "File \u001b[1;32mD:\\DataScienceAnywhere\\Udemy\\bert_tutorial\\bert_venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "        inputs,\n",
    "        labels,\n",
    "        batch_size=8,\n",
    "        epochs=10,\n",
    "    callbacks=[tensorboard_callback]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e9375f-736b-4026-8d5c-6149f1317270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "03b4415a-ba9b-443e-b0cd-9388af5809d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_masked_lm_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  90092544  \n",
      "                                                                 \n",
      " mlm___cls (TFBertMLMHead)   multiple                  5636252   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 90690716 (345.96 MB)\n",
      "Trainable params: 90690716 (345.96 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3da362f-3a00-499b-acb1-90524fc29345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6df28d1-ebca-41af-af52-a364695e3448",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7caee3-0be5-49c6-8525-09093acd1c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "5945955b-7e8a-4a31-9a40-9b990dfdf398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define the loss function and compile the model\n",
    "def compile_model(model):\n",
    "    loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "    model.compile(optimizer=optimizer, loss=loss_function)\n",
    "    return model\n",
    "\n",
    "# Step 4: Training the model\n",
    "def train_model(model, inputs, labels, batch_size=16, num_epochs=3):\n",
    "    model.fit(\n",
    "        inputs,\n",
    "        labels,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Step 5: Save the trained model and tokenizer\n",
    "def save_model_and_tokenizer(model, tokenizer, save_dir):\n",
    "    model.save_pretrained(save_dir)\n",
    "    tokenizer.save_pretrained(save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9319480f-694c-48ff-8d67-bbfa1b2f81fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
